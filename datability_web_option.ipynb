{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# データビリティweb講座 画像処理 OpenCVチュートリアル\n",
    "\n",
    "OpenCVを使った画像処理の演習です。まずはこのチュートリアルで基礎の基礎を学んでいきましょう。\n",
    "ちなみに、OpenCVの公式チュートリアルが https://docs.opencv.org/4.x/d9/df8/tutorial_root.html にあるので、こちらも活用してください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "---\n",
    "## 0. 準備\n",
    "インストールについては```README.md```を参照してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np  # PythonのOpenCVでは、画像はnumpyのarrayとして管理される\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 以下の設定はJuyter notebook向け（普通のPythonの場合はいらない）\n",
    "from IPython.display import display\n",
    "%matplotlib inline\n",
    "\n",
    "# 画像表示用の関数（jupyter notebookでインラインで表示したい時向け）\n",
    "def imshow_inline(img):\n",
    "    if img.ndim == 3:\n",
    "        img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB) # OpenCVの画像は、BGRの順に色が並んでいるので\n",
    "        display(Image.fromarray(img))\n",
    "    else:\n",
    "        display(Image.fromarray(img))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "---\n",
    "## 1. 画像の読み込み・書き込み編\n",
    "\n",
    "注意：表示されたウインドウは、**なにかキーが押されるまで残ります**（waitKey関数でキー入力を待っています）。終了するには、ウインドウをアクティブにしてなにかキーを押してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 画像ファイルの読み込み・書き込み\n",
    "img = cv2.imread(\"sample/sample.jpg\")\n",
    "# img = cv2.imread(\"sample/sample.jpg\",cv2.IMREAD_GRAYSCALE)  # 強制的にグレースケール（白黒画像）として読み込む場合\n",
    "\n",
    "# てきとうな処理（リサイズしてみた）\n",
    "#dst = cv2.resize(img,dsize=None,fx=0.5,fy=0.5)  # 縦横半分\n",
    "dst = cv2.resize(img,dsize=(512,256))  # 指定したサイズ\n",
    "\n",
    "# 画像を保存する場合\n",
    "# cv2.imwrite(\"out.png\",dst)\n",
    "\n",
    "# Jupyterでインライン表示する場合\n",
    "#imshow_inline(img)\n",
    "#imshow_inline(dst)\n",
    "\n",
    "# 新しいウインドウを開いて表示する場合（ウインドウを閉じるか、なにかキーを押すと終了）\n",
    "cv2.namedWindow('src') # 指定されたタイトルのウインドウを開く\n",
    "cv2.imshow('src',img)  # 指定されたタイトルのウインドウに画像を表示\n",
    "\n",
    "cv2.namedWindow('dst') # 指定されたタイトルのウインドウを開く\n",
    "cv2.imshow('dst',dst)  # 指定されたタイトルのウインドウに画像を表示\n",
    "\n",
    "cv2.waitKey(0)           # キーが押されるまで{引数}[ms]の間待つ（0の場合はずっと待つ）\n",
    "cv2.destroyAllWindows()  # ウインドウを閉じる。!!!jupyter上の場合、これを忘れるとフリーズする!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意：下記のウインドウは**'q'かESCキーが押されるまで残ります**（コード中で、waitKeyでキー入力を待ち、入力が'q'かESCだったらウインドウを閉じる、としています）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 動画の読み込み\n",
    "cap = cv2.VideoCapture('sample/sample.avi')\n",
    "\n",
    "end_flag, frame = cap.read()  # 最初のフレームを読み込み\n",
    "height, width, channels = frame.shape\n",
    "\n",
    "cv2.namedWindow('image') # 'image'というタイトルのウインドウを開く\n",
    "\n",
    "while end_flag == True:\n",
    "    cv2.imshow('image',frame)  # 'image'というタイトルのウインドウに画像を表示\n",
    "\n",
    "    key = cv2.waitKey(30) # キーが押されるまで{引数}[ms]の間待つ（0の場合はずっと待つ）\n",
    "    if key == 27 or key == ord('q'):  # qかESCが押されたら途中で終了\n",
    "        break\n",
    "\n",
    "    end_flag, frame = cap.read() # 次のフレームを読み込み\n",
    "\n",
    "cv2.destroyAllWindows()  # ウインドウを閉じる\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# webカメラの読み込み\n",
    "# !!!注意!!! 当然ですが、カメラがつながっていないとエラーになります\n",
    "\n",
    "# 引数はデバイスの順番。複数のカメラがつながっている場合は1などに変えると変わる\n",
    "cap = cv2.VideoCapture(0)\n",
    "#cap = cv2.VideoCapture(1)\n",
    "\n",
    "end_flag, frame = cap.read()  # 最初のフレームを読み込み\n",
    "height, width, channels = frame.shape\n",
    "\n",
    "cv2.namedWindow('image') # 'image'というタイトルのウインドウを開く\n",
    "\n",
    "while end_flag == True:\n",
    "    cv2.imshow('image',frame)  # 'image'というタイトルのウインドウに画像を表示\n",
    "\n",
    "    key = cv2.waitKey(30) # キーが押されるまで{引数}[ms]の間待つ（0の場合はずっと待つ）\n",
    "    if key == 27 or key == ord('q'):  # qかESCが押されたら途中で終了\n",
    "        break\n",
    "\n",
    "    end_flag, frame = cap.read() # 次のフレームを読み込み\n",
    "\n",
    "cv2.destroyAllWindows()  # ウインドウを閉じる\n",
    "cap.release()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "---\n",
    "## 2. 画素へのアクセスと色の操作編\n",
    "\n",
    "Tips:\n",
    "- 画像は配列。PythonのOpenCVでは、画像はnumpyの配列として保持される。\n",
    "- 画像のサイズを縦h, 横w, チャンネル数cで表すと、\n",
    "    - グレースケール（1ch）画像の場合、(h,w)の2次元のnumpy配列\n",
    "    - カラー画像（3ch, BGRの順で色が並ぶ）の場合、(h,w,c)の3次元のnumpy配列\n",
    "        - カラー画像の画素のチャンネルは、それぞれ青・緑・赤の色の強さを表す。\n",
    "- 原点は左上。縦軸をy、横軸をxとすると、左からx、上からy画素目の画素へのアクセスは`img[y,x] (img[y][x]でもOK)`（1ch画像の場合）\n",
    "- 配列の各要素には0~255の8bit (uint8)が格納されており、大きい値が明るい（あるいは色が強い）画素を表す。\n",
    "- 画像全体の画素に対する操作は、for文の2重ループ（それぞれ縦横の座標を表す）で走査する方法がある\n",
    "    - Pythonのfor文は遅いので、numpyやOpenCVの関数を使った効率化を積極的に検討すると良い。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# グレースケール画像の操作\n",
    "img_color = cv2.imread(\"sample/sample.jpg\")\n",
    "print(\"color (BGR): \",img_color.shape, \", type: \", img_color.dtype)\n",
    "\n",
    "img_gray = cv2.cvtColor(img_color,cv2.COLOR_BGR2GRAY) # グレイスケールに変換\n",
    "print(\"grayscale: \",img_gray.shape, \", type: \", img_color.dtype)\n",
    "\n",
    "# 各画素をスキャンして、条件にあった画素だけ書き換える（効率悪い。おそい。）\n",
    "out1 = img_gray.copy()\n",
    "out2 = img_gray.copy()\n",
    "for y in range(img_gray.shape[0]):\n",
    "    for x in range(img_gray.shape[1]):\n",
    "\n",
    "        # 例1: 画素値が128よりも小さい場合に全部255（白）にする\n",
    "        if out1[y,x] < 128:\n",
    "            out1[y,x] = 255\n",
    "\n",
    "        # 例2: y座標が30以上110未満の部分を黒塗り\n",
    "        if 30 <= y < 110:\n",
    "            out2[y,x] = 0\n",
    "\n",
    "# 例1は、numpyを使って以下のようにも実現できる。はやい。\n",
    "out_numpy = np.where(img_gray<128, 255, img_gray)\n",
    "\n",
    "# 例2は、numpyの配列操作で以下のように書ける。はやい。\n",
    "out2_numpy = img_gray.copy()\n",
    "out2_numpy[30:110,:] = 0\n",
    "\n",
    "cv2.imshow('gray',img_gray)\n",
    "cv2.imshow('out1',out1)\n",
    "cv2.imshow('out2',out2)\n",
    "\n",
    "cv2.waitKey(0)           # キーが押されるまで{引数}[ms]の間待つ（0の場合はずっと待つ）\n",
    "cv2.destroyAllWindows()  # ウインドウを閉じる。!!!jupyter上の場合、これを忘れるとフリーズする!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# カラー画像の操作\n",
    "img = cv2.imread(\"sample/sample.jpg\")\n",
    "\n",
    "out = np.zeros((img.shape[0],img.shape[1],img.shape[2]),np.uint8)   # 全画素0の画像\n",
    "for y in range(img.shape[0]):\n",
    "    for x in range(img.shape[1]):\n",
    "\n",
    "        # R（赤成分）だけ大きい部分のみ画素値を残し、その他は黒(0,0,0)にする\n",
    "        if img[y,x,0] < 128 and img[y,x,1] < 128 and img[y,x,2] > 128:\n",
    "            out[y,x,0] = img[y,x,0]\n",
    "            out[y,x,1] = img[y,x,1]\n",
    "            out[y,x,2] = img[y,x,2]\n",
    "        else:\n",
    "            out[y,x,0] = 0\n",
    "            out[y,x,1] = 0\n",
    "            out[y,x,2] = 0\n",
    "\n",
    "# OpenCVとnumpyを使って以下のようにも書ける\n",
    "mask = cv2.inRange(img, np.array([0,0,128]), np.array([128,128,255]))\n",
    "mask = np.dstack((mask,mask,mask))  # 3chにしたいので\n",
    "out_numpy = cv2.bitwise_and(img, mask)\n",
    "#out_numpy = img * (mask/255).astype(np.uint8)  # これでも良い\n",
    "\n",
    "cv2.imshow('img',img)\n",
    "cv2.imshow('out',out)\n",
    "cv2.imshow('out_numpy',out_numpy)\n",
    "\n",
    "cv2.waitKey(0)           # キーが押されるまで{引数}[ms]の間待つ（0の場合はずっと待つ）\n",
    "cv2.destroyAllWindows()  # ウインドウを閉じる。!!!jupyter上の場合、これを忘れるとフリーズする!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# カラー画像の操作（HSV編）\n",
    "img = cv2.imread(\"sample/sample.jpg\")\n",
    "\n",
    "# 色の抽出には、HSV色空間が直感的で適している。\n",
    "## H (Hue): 色相（本来は0~359度の色相環だが、画素値の上限から2で割って0~179になる。0度で赤。）\n",
    "## S (Saturation): 彩度（大きいほうが鮮やか。0~255）\n",
    "## V (Value): 明度（大きいほど明るい。0~255）\n",
    "img_hsv = cv2.cvtColor(img,cv2.COLOR_BGR2HSV)\n",
    "\n",
    "mask = np.zeros((img_hsv.shape[0],img_hsv.shape[1]),np.uint8)   # 全画素0のマスクをつくった\n",
    "for y in range(img_hsv.shape[0]):\n",
    "    for x in range(img_hsv.shape[1]):\n",
    "\n",
    "        # 赤い部分（Hが0~15か165~179の領域）を選ぶ\n",
    "        # 彩度と明度はある程度でかいほうが良い、とした例。\n",
    "        if (img_hsv[y,x,0] <= 15 and img_hsv[y,x,1] > 160 and img_hsv[y,x,2] > 128) or \\\n",
    "                (img_hsv[y,x,0] >= 165 and img_hsv[y,x,1] > 160 and img_hsv[y,x,2] > 128):\n",
    "            mask[y,x] = 255   # 「赤い」部分だけ255になるマスク\n",
    "\n",
    "mask = np.dstack((mask,mask,mask))  # 3chにしたいので\n",
    "out = cv2.bitwise_and(img, mask)  # マスクをかける\n",
    "\n",
    "cv2.imshow('img',img)\n",
    "cv2.imshow('out',out)\n",
    "\n",
    "cv2.waitKey(0)           # キーが押されるまで{引数}[ms]の間待つ（0の場合はずっと待つ）\n",
    "cv2.destroyAllWindows()  # ウインドウを閉じる。!!!jupyter上の場合、これを忘れるとフリーズする!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "---\n",
    "## 3. 画像の統計量\n",
    "\n",
    "画像のヒストグラム（度数分布）を計算するなどが代表的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "img = cv2.imread(\"sample/sample.jpg\")\n",
    "\n",
    "hist_B = cv2.calcHist([img],channels=[0],mask=None,histSize=[256],ranges=[0,256])  # B:0, G:1, R:2\n",
    "hist_G = cv2.calcHist([img],channels=[1],mask=None,histSize=[256],ranges=[0,256])  # B:0, G:1, R:2\n",
    "hist_R = cv2.calcHist([img],channels=[2],mask=None,histSize=[256],ranges=[0,256])  # B:0, G:1, R:2\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "x = range(0,256)\n",
    "x_axis = np.array(x)\n",
    "ax.plot(x_axis, np.array(hist_B), label='B', color='blue')\n",
    "ax.plot(x_axis, np.array(hist_G), label='G', color='green')\n",
    "ax.plot(x_axis, np.array(hist_R), label='R', color='red')\n",
    "ax.legend(loc=0)    # 凡例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "---\n",
    "## 4. 画像のフィルタリング\n",
    "\n",
    "画像の編集や特徴の抽出に使える。ここでは、画像を「ぼかす」例と、「エッジ」を抽出する例を紹介する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 画像を「ぼかす」\n",
    "\n",
    "img = cv2.imread(\"sample/wani.png\")\n",
    "dst = cv2.GaussianBlur(img,ksize=(21,21),sigmaX=7)\n",
    "\n",
    "cv2.imshow('input',img)\n",
    "cv2.imshow('blurred',dst)\n",
    "\n",
    "cv2.waitKey(0)           # キーが押されるまで{引数}[ms]の間待つ（0の場合はずっと待つ）\n",
    "cv2.destroyAllWindows()  # ウインドウを閉じる。!!!jupyter上の場合、これを忘れるとフリーズする!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 画像の変化量（勾配）を計算する\n",
    "\n",
    "img = cv2.imread(\"sample/prims.png\")\n",
    "img_gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY) # グレイスケールに変換\n",
    "\n",
    "grad_x = cv2.Sobel(img_gray,cv2.CV_64F,1,0,ksize=5) # x方向の変化の大きな点（縦方向のエッジ）を検出\n",
    "grad_y = cv2.Sobel(img_gray,cv2.CV_64F,0,1,ksize=5) # y方向の変化の大きな点（横方向のエッジ）を検出\n",
    "\n",
    "# 勾配（画素値の変化）の強さと方向を計算する。angle: [0,360)\n",
    "magnitude, angle = cv2.cartToPolar(grad_x, grad_y,angleInDegrees=True)\n",
    "\n",
    "cv2.imshow('input',img_gray)\n",
    "\n",
    "print(\"gradient magnitude and direction\")\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(magnitude)\n",
    "fig.colorbar(im, ax=ax)\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(angle)\n",
    "fig.colorbar(im, ax=ax)\n",
    "plt.show()\n",
    "\n",
    "cv2.waitKey(0)           # キーが押されるまで{引数}[ms]の間待つ（0の場合はずっと待つ）\n",
    "cv2.destroyAllWindows()  # ウインドウを閉じる。!!!jupyter上の場合、これを忘れるとフリーズする!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 画像のエッジを抽出する\n",
    "img = cv2.imread(\"sample/prims.png\")\n",
    "img_gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY) # グレイスケールに変換\n",
    "\n",
    "# 勾配計算の応用として、エッジっぽい点のみを検出する手法もある。「エッジっぽい点」のみ画素値を255、それ以外を0とした画像が出力される。\n",
    "edge_canny = cv2.Canny(img_gray,80,30)  # 第2、第3引数はしきい値。小さくするほど細かいエッジが出る。\n",
    "\n",
    "cv2.imshow('edge image',edge_canny)\n",
    "\n",
    "# 抽出されたエッジを使って直線を検出することもできる。\n",
    "draw = img.copy()\n",
    "lines = cv2.HoughLinesP(edge_canny,1.0,np.pi/180.0,30,maxLineGap=10)\n",
    "for line in lines:\n",
    "    x1,y1,x2,y2 = line[0]   # 始点(x1,y1), 終点(x2,y2)の直線\n",
    "    cv2.line(draw,(x1,y1),(x2,y2),(0,0,255),2)\n",
    "\n",
    "cv2.imshow('lines',draw)\n",
    "\n",
    "cv2.waitKey(0)           # キーが押されるまで{引数}[ms]の間待つ（0の場合はずっと待つ）\n",
    "cv2.destroyAllWindows()  # ウインドウを閉じる。!!!jupyter上の場合、これを忘れるとフリーズする!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "---\n",
    "## 5. 部分画像の操作\n",
    "numpyの要素アクセス方法をうまく使うと、画像の一部分（region of interest (ROI)）を取り出して簡単に処理できる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "img = cv2.imread(\"sample/sample.jpg\")\n",
    "\n",
    "img_roi = img[30:110,50:450,:]  #yが30~110, xが50~450, cは全部、の領域を取り出す\n",
    "img[30:110,50:450,:] = cv2.GaussianBlur(img_roi,ksize=(21,21),sigmaX=7) # ぼかして、元の領域に上書きする\n",
    "\n",
    "cv2.imshow('img_roi',img_roi)\n",
    "cv2.imshow('img',img)\n",
    "\n",
    "cv2.waitKey(0)           # キーが押されるまで{引数}[ms]の間待つ（0の場合はずっと待つ）\n",
    "cv2.destroyAllWindows()  # ウインドウを閉じる。!!!jupyter上の場合、これを忘れるとフリーズする!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "---\n",
    "## 6. 画像の変形\n",
    "\n",
    "座標(x,y)を操作することで、画像を変形（ワーピング）することができる。詳しくは、射影変換（ホモグラフィ変換）を調べてみよう\n",
    "\n",
    "以下は、2つの画像を張り合わせる例。4組の対応点を指定し、その前後の変換（ホモグラフィ行列）を計算して変形する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# imgをrefに張り合わせることを考える\n",
    "ref = cv2.imread(\"sample/pano_ref.jpg\") # ベースとなる画像（BGR）\n",
    "src = cv2.imread(\"sample/pano_src.jpg\") # 変換する画像（BGR）\n",
    "\n",
    "cv2.imshow('ref',ref)\n",
    "cv2.imshow('src',src)\n",
    "\n",
    "pts_ref = np.float32([[923,156],[1281,143],[1276,760],[916,745]]) # refの点(x', y')\n",
    "pts_src = np.float32([[88,163],[438,190],[437,760],[78,782]])  # srcの点(x, y)\n",
    "\n",
    "# 横幅2倍の画像を作って、そこに貼り付けることにした。配列作成時のshapeの設定はw,h,cの順なので注意\n",
    "dst = np.zeros((src.shape[0],src.shape[1]*2,3))\n",
    "dst[:,0:ref.shape[1],:] = ref # 左半分にrefをコピー\n",
    "#H = cv2.getPerspectiveTransform(pts_src,pts_ref)  # 最小二乗法によるホモグラフィ行列の推定（img -> refへの変換）　誤対応がない場合はこちらもOK\n",
    "H, mask = cv2.findHomography(pts_src, pts_ref, cv2.RANSAC, 3.0) # 外れ値除去（RANSAC）つきホモグラフィ行列の推定（img -> refへの変換）\n",
    "\n",
    "src_warped = cv2.warpPerspective(src, H, (src.shape[1]*2,src.shape[0]), flags=cv2.INTER_NEAREST) # ホモグラフィ行列を使ったワーピング\n",
    "\n",
    "mask = np.where(src_warped==0,0,1).astype(np.uint8) # 変換後srcの値がある場所が1になるマスク画像を作る\n",
    "dst = dst*(1-mask) + src_warped*mask  # アルファブレンディング\n",
    "dst = dst.astype(np.uint8)  # uint8に変換\n",
    "\n",
    "cv2.imshow('dst',dst)\n",
    "\n",
    "cv2.waitKey(0)           # キーが押されるまで{引数}[ms]の間待つ（0の場合はずっと待つ）\n",
    "cv2.destroyAllWindows()  # ウインドウを閉じる。!!!jupyter上の場合、これを忘れるとフリーズする!!!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "---\n",
    "## 7. 画像の対応付け\n",
    "\n",
    "画像の同じ部分はどこか？を探したい。そんな時は対応付けが便利。\n",
    "画像上の特徴的な点を探し、それら同士のマッチングを行っている。\n",
    "\n",
    "https://docs.opencv.org/4.x/db/d27/tutorial_py_table_of_contents_feature2d.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# imgとref間のマッチングをする\n",
    "ref = cv2.imread(\"sample/pano_ref.jpg\")\n",
    "src = cv2.imread(\"sample/pano_src.jpg\")\n",
    "\n",
    "# ここでは、ORBという特徴点・特徴量を使ってみる。\n",
    "orb = cv2.ORB_create()\n",
    "\n",
    "# 各画像に対する特徴点（keypoint）とそれに付随する特徴量（descriptor）の計算\n",
    "kp_ref, des_ref = orb.detectAndCompute(ref, None)\n",
    "kp_src, des_src = orb.detectAndCompute(src, None)\n",
    "\n",
    "# マッチング。特徴量の性質から、ハミング距離を使う。\n",
    "matcher = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "matches = matcher.match(des_ref,des_src)\n",
    "\n",
    "# 全対応を可視化。対応づいた点同士が線で結ばれる。\n",
    "corr_disp = cv2.drawMatches(ref,kp_ref,src,kp_src,matches,None,flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "cv2.imshow('correspondences',corr_disp) # 表示\n",
    "\n",
    "cv2.waitKey(0)           # キーが押されるまで{引数}[ms]の間待つ（0の場合はずっと待つ）\n",
    "cv2.destroyAllWindows()  # ウインドウを閉じる。!!!jupyter上の場合、これを忘れるとフリーズする!!!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "---\n",
    "## 8. ユーザ入力の取得・描画\n",
    "OpenCVでは、namedWindow上でのマウスやキーボード入力の取得が可能\n",
    "\n",
    "また、画像上への描画関数も実装されている。丸以外に、直線（```cv2.line```）、長方形（```cv2.rectangle```）なども。\n",
    "\n",
    "画像上での文字描画も可能。興味があれば調べてみよう（```cv2.putText```）\n",
    "\n",
    "注意：ウインドウは**'q'かESCキーが押されるまで残ります**（コード中で、waitKeyでキー入力を待ち、入力が'q'かESCだったらウインドウを閉じる、としています）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "img = cv2.imread(\"sample/sample.jpg\")\n",
    "draw = img.copy()\n",
    "\n",
    "# マウスコールバック関数。マウスイベントが発生すると呼ばれる\n",
    "def mouse_callback(event, x, y, flags, param):\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        cv2.circle(draw,(x,y),10,(0,255,0), 2)\n",
    "        print(\"left press: (\", x, \",\", y, \")\")\n",
    "    if event == cv2.EVENT_RBUTTONDOWN:\n",
    "        cv2.circle(draw,(x,y),10,(255,0,0), 2)\n",
    "        print(\"right press: (\", x, \",\", y, \")\")\n",
    "    if event == cv2.EVENT_LBUTTONUP:\n",
    "        print(\"left up: (\", x, \",\", y, \")\")\n",
    "    if event == cv2.EVENT_RBUTTONUP:\n",
    "        print(\"right up: (\", x, \",\", y, \")\")\n",
    "\n",
    "# トラックバーのコールバック関数。トラックバーが動くと呼ばれる\n",
    "def trackbar_callback(pos):\n",
    "    print(\"trackbar: \", pos)\n",
    "\n",
    "cv2.namedWindow(\"window\")\n",
    "cv2.setMouseCallback(\"window\", mouse_callback)  # マウスコールバック関数の設定\n",
    "cv2.createTrackbar(\"trackbar1\",\"window\",0,100,trackbar_callback) # トラックバーの追加\n",
    "\n",
    "key = 0\n",
    "while True:\n",
    "    cv2.imshow(\"window\",draw)  # 'image'というタイトルのウインドウに画像を表示\n",
    "\n",
    "    # キーボード入力はcv2.waitKeyで取得する\n",
    "    key = cv2.waitKey(30) # キーが押されるまで{引数}[ms]の間待つ（0の場合はずっと待つ）\n",
    "    if key == 27 or key == ord('q'):  # qかESCが押されたら途中で終了\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()  # ウインドウを閉じる\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "---\n",
    "## 9. 機械学習による物体検出\n",
    "ここでは、学習済みHaar-Cascade特徴を使った顔検出を紹介する。それ以外の学習済み特徴も、`haarcascades`ディレクトリ以下にある。\n",
    "目鼻口などもあるので遊んでみると良い。\n",
    "\n",
    "https://docs.opencv.org/4.x/db/d28/tutorial_cascade_classifier.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 学習済みファイルの入力\n",
    "face_cascade_file  = \"sample/haarcascades/haarcascade_frontalface_default.xml\"\n",
    "face_cascade = cv2.CascadeClassifier(face_cascade_file)\n",
    "\n",
    "img = cv2.imread(\"sample/face.png\")\n",
    "img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "face_list = face_cascade.detectMultiScale(img_gray, minSize=(30, 30))   # 顔検出\n",
    "#print(face_list)\n",
    "\n",
    "for (x, y, w, h) in face_list:  # 検出した顔\n",
    "    cv2.rectangle(img, (x, y), (x + w, y + h), (0, 0, 225), thickness = 3)  # 顔の枠を描く\n",
    "\n",
    "cv2.imshow('img',img) # 表示\n",
    "\n",
    "cv2.waitKey(0)           # キーが押されるまで{引数}[ms]の間待つ（0の場合はずっと待つ）\n",
    "cv2.destroyAllWindows()  # ウインドウを閉じる。!!!jupyter上の場合、これを忘れるとフリーズする!!!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "---\n",
    "## 10. 深層学習\n",
    "\n",
    "深層学習を使うと、より精度の高い物体検出や、精度の高い領域分割など、様々な応用が実現できる。\n",
    "OpenCVでは、一般的な深層学習ライブラリ（Pytorchなど）の学習済みモデルを読み込んで実行できる（が、多くの場合、あえてOpenCVを通して実行する必要はない）。\n",
    "\n",
    "環境の制約（GPUが必要な場合が多い）などがあるので、興味のある学生は各自以下などから調べてみると良い。\n",
    "\n",
    "https://github.com/opencv/opencv/tree/4.x/samples/dnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "---\n",
    "## Next Step\n",
    "`08D12345.py`に課題実装用テンプレートがあるので、なにか作ってみよう。\n",
    "\n",
    "### FAQ\n",
    "\n",
    "**何を作ればよいか思いつかない**\n",
    "\n",
    "`doc`ディレクトリ以下に、ちょうど良さそうな例をいくつか紹介したスライドを用意したので、それらに取り組んでも良い。\n",
    "ただし、それらにとらわれず、自由な発想で取り組むことをおすすめする。\n",
    "\n",
    "**どれくらいの規模感のものを作れば良い？**\n",
    "\n",
    "小さいもので構わない。上記のチュートリアルの内容を組み合わせた、あるいは少々書き換えただけのようなものでも、その有用性や実装内容がレポートで十分に説明されていれば問題ない。\n",
    "ちなみに、`tutorial.ipynb`はあくまで参考資料の一つ、という立ち位置である。\n",
    "よって、チュートリアルに記載されたコードを使う場合、それ自体の説明もきちんとレポート中に含めること。\n",
    "\n",
    "**こういうのが作りたいんだけど、どのように実装すれば良いかわからない**\n",
    "\n",
    "大倉（[okura@ist.osaka-u.ac.jp](okura@ist.osaka-u.ac.jp)）までお気軽にご相談を。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
